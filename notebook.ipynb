{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Introduction to ML - Decision Tree Coursework</center>\n",
    "### <center>COMP70050</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "clean_data = x = np.loadtxt(\"wifi_db/clean_dataset.txt\", delimiter='\\t')\n",
    "noisy_data = x = np.loadtxt(\"wifi_db/noisy_dataset.txt\")\n",
    "\n",
    "X_clean = clean_data[:, :-1]\n",
    "y_clean = clean_data[:, -1]\n",
    "\n",
    "X_noisy = noisy_data[:, :-1]\n",
    "y_noisy = noisy_data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(labels):\n",
    "    _, counts = np.unique(labels, return_counts=True)\n",
    "    prob = counts / np.sum(counts)\n",
    "    return -np.sum(prob * np.log2(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_split(dataset):\n",
    "        # We split out dataset, x is the array of features and y is the labels\n",
    "        x = dataset[:, :-1]\n",
    "        y = dataset[:, -1]\n",
    "\n",
    "        # n is the number of samples and k is the number of features\n",
    "        n, k = x.shape\n",
    "\n",
    "        H_total = entropy(y)\n",
    "    \n",
    "        max_gain = 0\n",
    "        best_attribute = -1\n",
    "        best_split = None\n",
    "\n",
    "        for attribute in range(k):\n",
    "            arr = x[:, attribute]\n",
    "            uniques = np.unique(arr)\n",
    "\n",
    "            for split in uniques:\n",
    "                left_ds = y[arr <= split]\n",
    "                right_ds = y[arr > split]\n",
    "\n",
    "                remainder = ((len(left_ds)/len(arr)) * entropy(left_ds)) + ((len(right_ds)/len(arr)) * entropy(right_ds))\n",
    "                \n",
    "                if max_gain < H_total - remainder:\n",
    "\n",
    "                    max_gain = H_total - remainder\n",
    "                    best_attribute = attribute\n",
    "                    best_split = split\n",
    "        \n",
    "        return best_attribute, best_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_learning(dataset, depth=0):\n",
    "    total_instances = len(dataset)\n",
    "    x = dataset[:, :-1]\n",
    "    y = dataset[:, -1]\n",
    "    \n",
    "    if len(np.unique(y)) == 1:\n",
    "        return (Node(None, y[0], None, None, total_instances, True), depth)\n",
    "    else:\n",
    "        attribute, value = find_split(dataset)\n",
    "        l_dataset = dataset[dataset[:, attribute] <= value]\n",
    "        r_dataset = dataset[dataset[:, attribute]  > value]\n",
    "        l_node, l_depth = decision_tree_learning(l_dataset, depth + 1)\n",
    "        r_node, r_depth = decision_tree_learning(r_dataset, depth + 1)\n",
    "        node = Node(attribute, value, l_node, r_node, total_instances, False)\n",
    "        return (node, max(l_depth, r_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, attribute, value, left, right, n_instances: int, is_leaf: bool):\n",
    "        self.attribute = attribute\n",
    "        self.value = value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.n_instances = n_instances\n",
    "        self.is_leaf = is_leaf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(node, X):\n",
    "    if node.is_leaf:\n",
    "        return np.full(len(X), node.value)\n",
    "    \n",
    "    left_indices = X[:, node.attribute] <= node.value\n",
    "    right_indices = X[:, node.attribute] > node.value\n",
    "    \n",
    "    predictions  = np.zeros(len(X))\n",
    "    predictions[left_indices] = predict(node.left, X[left_indices])\n",
    "    predictions[right_indices] = predict(node.right, X[right_indices])\n",
    "\n",
    "    return predictions  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_gold, y_prediction, class_labels=None):\n",
    "\n",
    "    # if no class_labels are given, we obtain the set of unique class labels from\n",
    "    # the union of the ground truth annotation and the prediction\n",
    "    if not class_labels:\n",
    "        class_labels = np.unique(np.concatenate((y_gold, y_prediction)))\n",
    "\n",
    "    confusion = np.zeros((len(class_labels), len(class_labels)), dtype=np.int8)\n",
    "\n",
    "    # for each correct class (row),\n",
    "    # compute how many instances are predicted for each class (columns)\n",
    "    for i, correct_class in enumerate(class_labels):\n",
    "        for j, predicted_class in enumerate(class_labels):\n",
    "            count = np.count_nonzero(np.logical_and(y_gold == correct_class, y_prediction == predicted_class))\n",
    "            confusion[i][j] = count\n",
    "\n",
    "    return confusion\n",
    "\n",
    "def accuracy(confusion):\n",
    "    return np.trace(confusion) / np.sum(confusion) if np.sum(confusion) > 0 else 0\n",
    "\n",
    "\n",
    "def precision(confusion):\n",
    "\n",
    "    # Compute the precision per class\n",
    "    p = np.diag(confusion) / np.sum(confusion, axis=1)\n",
    "\n",
    "    # Compute the macro-averaged precision\n",
    "    macro_p = np.mean(p)\n",
    "\n",
    "    return (p, macro_p)\n",
    "\n",
    "def recall(confusion):\n",
    "\n",
    "    # Compute the recall per class\n",
    "    r = np.diag(confusion) / np.sum(confusion, axis=0)\n",
    "    # Compute the macro-averaged recall\n",
    "    macro_r = np.mean(r)\n",
    "    return (r, macro_r)\n",
    "\n",
    "def f1_score(confusion):\n",
    "\n",
    "\n",
    "    (precisions, macro_p) = precision(confusion)\n",
    "    (recalls, macro_r) = recall(confusion)\n",
    "\n",
    "    # just to make sure they are of the same length\n",
    "    assert len(precisions) == len(recalls)\n",
    "\n",
    "    f = (2*precisions*recalls) / (precisions + recalls)\n",
    "\n",
    "    macro_f = np.mean(f)\n",
    "\n",
    "    return (f, macro_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_k_fold(n_folds, n_instances, random_generator=np.random.default_rng()):\n",
    "\n",
    "    shuffled_indices = random_generator.permutation(n_instances)\n",
    "    split_indices = np.array_split(shuffled_indices, n_folds)\n",
    "\n",
    "    folds = []\n",
    "    for k in range(n_folds):\n",
    "        test_indices = split_indices[k]\n",
    "        train_indices = np.concatenate(split_indices[:k] + split_indices[k+1:])\n",
    "\n",
    "        folds.append([train_indices, test_indices])\n",
    "\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_dataset, trained_tree):\n",
    "    y_pred = predict(trained_tree, test_dataset[:, :-1])\n",
    "    \n",
    "    accuracy = np.mean(y_pred == test_dataset[:, -1])\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_tree(node: Node, train_dataset, val_dataset):\n",
    "    \n",
    "    if node.is_leaf:\n",
    "        return node\n",
    "\n",
    "    left_train_dataset = train_dataset[train_dataset[:, node.attribute] <= node.value]\n",
    "    right_train_dataset = train_dataset[train_dataset[:, node.attribute] > node.value]\n",
    "    \n",
    "    node.left = prune_tree(node.left, left_train_dataset, val_dataset)\n",
    "    node.right = prune_tree(node.right, right_train_dataset, val_dataset)\n",
    "    \n",
    "    if node.left.is_leaf and node.right.is_leaf:\n",
    "\n",
    "        new_value = node.left.value if node.left.n_instances > node.right.n_instances else node.right.value\n",
    "        potential_leaf = Node(None, new_value, None, None, len(train_dataset), True)\n",
    "\n",
    "        if evaluate(val_dataset, node) <= evaluate(val_dataset, potential_leaf):\n",
    "            return potential_leaf\n",
    "\n",
    "    return node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before pruning:\n",
      "Accuracy: 0.7979999999999999\n",
      "Precision: 0.7978295300553822\n",
      "Recall: 0.7996149848876579\n",
      "F1 Score: 0.7970156093650821\n",
      "\n",
      "After pruning:\n",
      "Accuracy: 0.8230000000000001\n",
      "Precision: 0.8225623770802253\n",
      "Recall: 0.8248185242951935\n",
      "F1 Score: 0.8222422039576095\n"
     ]
    }
   ],
   "source": [
    "dataset = noisy_data\n",
    "n_folds = 10\n",
    "n_instances = len(dataset)\n",
    "\n",
    "confusion_matrices = np.zeros((n_folds, 4, 4))\n",
    "accuracies = np.zeros(n_folds)\n",
    "precisions = np.zeros(n_folds)\n",
    "recalls = np.zeros(n_folds)\n",
    "f1_scores = np.zeros(n_folds)\n",
    "\n",
    "confusion_matrices_after = np.zeros((n_folds, 4, 4))\n",
    "accuracies_after = np.zeros(n_folds)\n",
    "precisions_after = np.zeros(n_folds)\n",
    "recalls_after = np.zeros(n_folds)\n",
    "f1_scores_after = np.zeros(n_folds)\n",
    "\n",
    "for i, (train_indices, test_indices) in enumerate(train_test_k_fold(n_folds, len(x))):\n",
    "    # Splitting the train and test\n",
    "    x_train = dataset[train_indices, :-1]\n",
    "    y_train = dataset[train_indices, -1]\n",
    "    x_test = dataset[test_indices, :-1]\n",
    "    y_test = dataset[test_indices, -1]\n",
    "\n",
    "    root, depth = decision_tree_learning(dataset[train_indices])\n",
    "    y_pred = predict(root, x_test)\n",
    "\n",
    "    root = prune_tree(root, dataset[train_indices], dataset[test_indices])\n",
    "    y_pred_after = predict(root, x_test)\n",
    "\n",
    "    confusion_matrices[i] = confusion_matrix(np.int8(y_test), np.int8(y_pred))\n",
    "    accuracies[i] = accuracy(confusion_matrices[i])\n",
    "    precisions[i] = precision(confusion_matrices[i])[1]\n",
    "    recalls[i] = recall(confusion_matrices[i])[1]\n",
    "    f1_scores[i] = f1_score(confusion_matrices[i])[1]\n",
    "    train_dataset = np.column_stack((x_test, y_test))    \n",
    "\n",
    "    confusion_matrices_after[i] = confusion_matrix(np.int8(y_test), np.int8(y_pred_after))\n",
    "    accuracies_after[i] = accuracy(confusion_matrices_after[i])\n",
    "    precisions_after[i] = precision(confusion_matrices_after[i])[1]\n",
    "    recalls_after[i] = recall(confusion_matrices_after[i])[1]\n",
    "    f1_scores_after[i] = f1_score(confusion_matrices_after[i])[1]\n",
    "    train_dataset_after = np.column_stack((x_test, y_test))  \n",
    "    \n",
    "print(\"Before pruning:\")\n",
    "print(f\"Accuracy: {accuracies.mean()}\")\n",
    "print(f\"Precision: {precisions.mean()}\")\n",
    "print(f\"Recall: {recalls.mean()}\")\n",
    "print(f\"F1 Score: {f1_scores.mean()}\")\n",
    "\n",
    "print(\"\\nAfter pruning:\")\n",
    "print(f\"Accuracy: {accuracies_after.mean()}\")\n",
    "print(f\"Precision: {precisions_after.mean()}\")\n",
    "print(f\"Recall: {recalls_after.mean()}\")\n",
    "print(f\"F1 Score: {f1_scores_after.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
